---
title: "ECO 395M Project"
author: "Zining Liu"
date: "2019/5/15"
output: github_document
---

```{r setup, include=FALSE, warning=FALSE, message=FALSE}
rm(list=ls())
library(rmarkdown)
knitr::opts_chunk$set(echo = FALSE)
```

```{r, include=FALSE, warning=FALSE, message=FALSE}

library(tidyverse)
library(dplyr)
library(lubridate)
library(tree)
library(MASS)
library(gbm)
library(randomForest)

setwd("~/Desktop/statistical learning/final/austin-outcomes")
# outcome<-read.csv("aac_shelter_outcomes.csv")
cat<-read.csv("aac_shelter_cat_outcome_eng.csv")
```


```{r, include=FALSE, warning=FALSE, message=FALSE}
#### Data Cleanning

# select useful variables
cat1<-cat %>% dplyr::select(date_of_birth,datetime,name,outcome_type,outcome_subtype,sex,Spay.Neuter,Cat.Kitten..outcome.,breed1,
                    breed2,cfa_breed,coat_pattern,color1,color2)

# remove observations without outcome (3)
cat1<-cat1[cat1$outcome_type!="",]

# create age at result date
cat1$date_of_birth<-ymd_hms(cat1$date_of_birth)
cat1$datetime<-ymd_hms(cat1$datetime)
cat1$interval<-cat1$date_of_birth %--% cat1$datetime
cat1$age_at.result<-as.duration(cat1$interval)
cat1$interval<-NULL
summary(cat1$age_at.result)


# change age from seconds to days, remove errors where age < 0
cat1$age_at.result <- cat1$age_at.result@.Data/(3600*24)
cat1 <- subset(cat1, age_at.result>0)


colnames(cat1)[colnames(cat1)=="Cat.Kitten..outcome."] <- "Cat.Kitten"



#### Feature Engineering

# get name dummy (if cat is named by staff)
cat1$name <- ifelse(cat1$name!="", 1, 0)

# Create result category: 
# 1=adopt, return to owner, rto adopt (return to owner adopt); 
# 2=die, euthanasia, disposal
# 3=transfer to partner shelter, release program, missing

cat1$result <- ifelse(cat1$outcome_type=="Adoption"|cat1$outcome_type=="Return to Owner"|cat1$outcome_type=="Rto-Adopt", "1", 
                      ifelse(cat1$outcome_type=="Died"|cat1$outcome_type=="Disposal"|cat1$outcome_type=="Euthanasia", "2", "3"))


# mix breed
cat1$breed_mix <- ifelse(cat1$breed2=="",0,1)


# color and coat pattern

# tricolor->tortie, brindle->tortie (usually viewed as tortie from human eyes), agouti->tabby (a subtype of tabby)
cat1$coat_pattern[cat1$coat_pattern=="tricolor"]<-"tortie"
cat1$coat_pattern[cat1$coat_pattern=="brindle"]<-"tortie"
cat1$coat_pattern[cat1$coat_pattern=="agouti"]<-"tabby"

# drop coat due to very high correlation with color1
cat1$coat<-NULL

# create below color type for color1 and color2
# 1 [white]
# 2 [light yellow ~ red]: cream, buff, yellow, apricot, fawn, lynx, tan, orange, orange tiger, pink, flame, red
# 3 [silver ~ blue]: silver, silver lynx, lilac, gray, blue cream, blue
# 4 [brown]: brown, chocolate, sable
# 5 [black]: seal, black
# 6 [mix]: breed specific mixed color

cat1$color1<-as.character(cat1$color1)
cat1$color2<-as.character(cat1$color2)

cat1$color2[which(cat1$color2=="white")]<-"1"

cat1$color2[which(cat1$color2=="apricot"|cat1$color2=="cream"|cat1$color2=="buff"|cat1$color2=="orange"|
                    cat1$color2=="red"|cat1$color2=="tan"|cat1$color2=="lynx"|cat1$color2=="flame"|
                    cat1$color2=="yellow")]<-"2"

cat1$color2[which(cat1$color2=="blue"|cat1$color2=="blue cream"|cat1$color2=="gray"|cat1$color2=="lilac"|
                    cat1$color2=="silver")]<-"3"

cat1$color2[which(cat1$color2=="brown"|cat1$color2=="chocolate")]<-"4"

cat1$color2[which(cat1$color2=="black"|cat1$color2=="seal")]<-"5"



cat1$color1[which(cat1$color1=="white")]<-"1"

cat1$color1[which(cat1$color1=="apricot"|cat1$color1=="buff"|cat1$color1=="cream"|cat1$color1=="cream "|cat1$color1=="fawn"|
                    cat1$color1=="flame"|cat1$color1=="flame "|cat1$color1=="lynx"|cat1$color1=="lynx "
                  |cat1$color1=="orange"|cat1$color1=="orange "|cat1$color1=="pink"|cat1$color1=="orange tiger"
                  |cat1$color1=="tan"|cat1$color1=="yellow")]<-"2"

cat1$color1[which(cat1$color1=="blue"|cat1$color1=="blue "|cat1$color1=="blue cream"|cat1$color1=="gray"|
                    cat1$color1=="gray "|cat1$color1=="lilac"|cat1$color1=="lilac "|cat1$color1=="silver"
                  |cat1$color1=="silver "|cat1$color1=="silver lynx"|cat1$color1=="silver lynx ")]<-"3"

cat1$color1[which(cat1$color1=="brown"|cat1$color1=="brown "|cat1$color1=="brown merle"|cat1$color1=="sable"
                  |cat1$color1=="brown tiger"|cat1$color1=="chocolate"|cat1$color1=="chocolate ")]<-"4"

cat1$color1[which(cat1$color1=="black"|cat1$color1=="black "|cat1$color1=="black tiger"|
                    cat1$color1=="seal"|cat1$color1=="seal ")]<-"5"

cat1$color1[which(cat1$color1=="Breed Specific")]<-"6"


# create breed_1 which put breeds into big categories according to the cat breed family tree

cat1$breed_1[which(cat1$breed1=="domestic longhair"|cat1$breed1=="domestic mediumhair"|cat1$breed1=="domestic shorthair")]<-"0"
cat1$breed_1[which(cat1$breed1=="siamese"|cat1$breed1=="birman"|cat1$breed1=="burmese"|cat1$breed1=="havana brown"|cat1$breed1=="japanese bobtail"
                   |cat1$breed1=="balinese"|cat1$breed1=="javanese"|cat1$breed1=="oriental sh"|cat1$breed1=="snowshoe"
                   |cat1$breed1=="tonkinese"|cat1$breed1=="javanese")]<-"1"


cat1$breed_1[which(cat1$breed1=="abyssinian"|cat1$breed1=="american shorthair"|cat1$breed1=="american curl shorthair"
                   |cat1$breed1=="american wirehair"|cat1$breed1=="exotic shorthair"|cat1$breed1=="maine coon"|cat1$breed1=="norwegian forest cat"
                   |cat1$breed1=="sphynx"|cat1$breed1=="ocicat"|cat1$breed1=="british shorthair"
                   |cat1$breed1=="scottish fold"|cat1$breed1=="russian blue"|cat1$breed1=="persian"|cat1$breed1=="chartreux"
                   |cat1$breed1=="himalayan")]<-"2"

cat1$breed_1[which(cat1$breed1=="cymric"|cat1$breed1=="manx")]<-"3"

cat1$breed_1[which(cat1$breed1=="ragdoll")]<-"4"

cat1$breed_1[which(cat1$breed1=="turkish angora"|cat1$breed1=="turkish van"|cat1$breed1=="angora")]<-"5"

cat1$breed_1[is.na(cat1$breed_1==TRUE)]<-"6"


# create dummy for long hair
cat1$long<-"0"
cat1$long[which(cat1$breed1=="angora"|cat1$breed1=="american curl"|cat1$breed1=="birman"|cat1$breed1=="domestic longhair"
                |cat1$breed1=="domestic mediumhair"|cat1$breed1=="balinese"|cat1$breed1=="cymric"|cat1$breed1=="himalayan"
                |cat1$breed1=="javanese"|cat1$breed1=="maine coon"|cat1$breed1=="munchkin longhair"|cat1$breed1=="norwegian forest cat"
                |cat1$breed1=="persian"|cat1$breed1=="ragdoll"|cat1$breed1=="turkish angora"|cat1$breed1=="turkish van")]<-"1"



# define categorical variables as factor
class(cat1$name)
cat1$name <- as.factor(cat1$name)
cat1$color1 <- as.factor(cat1$color1)
cat1$color2 <- as.factor(cat1$color2)
cat1$result <- as.factor(cat1$result)
cat1$breed_mix <- as.factor(cat1$breed_mix)
cat1$breed_1 <- as.factor(cat1$breed_1)
cat1$long <- as.factor(cat1$long)


cat1$outcome_subtype<-NULL
cat1$outcome_type<-NULL

```

## predicting adoption outcome for cats in Austin Animal Center Shelter   
    
&nbsp;
&nbsp;    


### Abstract   

In this project, we build a model to predict the adoption outcome of stray cats in Austin Animal Center Shelter. We use the decision tree and ensemble method to build the model and estimate the accuracy of prediction. Boosting works best in prediction if we allow a sufficiently large number of trees, while Bagging is nearly as good as Boosting, both of which yield a mean square error of approximately 10%. The most important factors that people consider when they adopt a cat from the shelter are age, neuter status, name, and probably main color, and coat pattern. Some limitation might include a very unbalanced breed frequency which might hinder us from getting a good estimate.   

&nbsp;
&nbsp;   

### Introduction   
   
&nbsp;
&nbsp;  

Animal shelter is a non-negligible and large-scale industry. Every year around 6.5 million companion animals enter animal shelters in the U.S. Around 3.2 million of the shelter animals are adopted each year, which is the most ideal result a shelter companion animal could get.
With a large number of shelter animals and turnover, these facilities can be costly to run. Most of centers rely on donation, government grants, crowdfunding, and adoption fee to keep their door open. Animal adoption will reduce the cost and bring profits through adoption fee. It is also one of the core functions of the shelter as a humanitarian facility. Thus, it is important to estimate the adoption result for shelter animals, as it helps the center to forecast cost based on adoption prediction and plan ahead to place the animals wisely.   


Austin Animal Center Shelter is the biggest animal shelter in Austin which shelters to nearly 20,000 animals. For the purpose of this study, we focus on the prediction of cat adoption result. The data was found on Kaggle, with original source from Austin Open Data Portal. Data contains the outcome of around 30000 shelter cats (adoption, transfer, euthanasia, release, etc.) from 2014 up to now as well as their relevant features, including age, sex, neuter status, breed, name, color and pattern, etc. The purpose is to build a model to predict adoption based on the features, estimate model accuracy and interpret the important factors.    

&nbsp;
&nbsp; 

### Method 

The dataset contains 29421 observations and 37 variables. Variables include id, birth time, sex, neuter, breed information, color and pattern information.    

Firstly, we cleaned the data to remove observation with missing results and errors. Secondly, we improved the quality of variables by feature engineering. We created age at outcome variable as the duration of the cat birth time to result time. We created dummy variable for cats with mixed breed (mixed from 2 breeds other than domestic breed). We put the 25 color into 5 categories: white, yellowish, blueish, brownish, black. We created dummy variable to separate long hair cat from the short. Since there are not enough variation for cats in breeds other than domestic short/longhair, we put all other breeds into one bin. Thus, breeds are now dummy variable to classify domestic (mixed) breed from the rest. We labelled result to be 1 (adoption, return to owner), 2 (die, euthanasia, disposal) and 3 (transfer, release).
To explore if there are variation in adoption rate for cats with different features, we visualize the adoption frequency as below.   


#### Figure 1  

```{r, echo=FALSE, warning=FALSE}
ggplot(cat1, aes(x=age_at.result, fill=result, color=result))+
  geom_histogram(position="identity", alpha=0.2, bins=100)+
  scale_x_continuous(limits= c(0,200))+
  labs(title="Age Histogram Plot at Result",x="age(days)", y = "Count",
       subtitle="adoption tends to happen at the age of 50~150 days")

```

#### Figure 2  

```{r, echo=FALSE}
ggplot(cat1, aes(x=sex, fill=result, color=result))+
  geom_bar(position="identity", alpha=0.2)+
  facet_grid(Cat.Kitten~Spay.Neuter, labeller=label_both)+
  labs(title="Neuter Status, Weaning Status, and Sex at Result")

```
&nbsp;
&nbsp;  

Figure 1 shows the difference in distribution of age for the three groups. The peak of adoption is when the cats are 50 days to 150 days of age, and the peak of death are below 50 days of age. Figure 2 shows the distribution of neuter status, whether cat is kitten or older, and sex for the three outcome groups. For unneutered group, adoption tends to happen to kitten and might happen more often to female. Adoption rate is much higher for neutered group no matter the age or gender. Within neutered group, kitten is more preferred and male kitten is the most preferred subgroup.   

There is indeed variation in feature variables across result groups. So theoretically we could use these variables to make prediction of the adoption result.    


Next, we use decision tree to form predictive model. We favor tree method in this case because people usually follow a tree-like process when they make decision of whether to adopt a certain animal or not. They have a list of requirements for companion animal. These requirements follow an ordinal scale so that some are priority, and some are less important. Therefore, we use tree to model this adoption decision, which can be used to predict current shelter animal outcome based on their features.   
&nbsp;
&nbsp;  


### Result   
&nbsp;
&nbsp;  

We re-label the result to be dummy variable denoting adoption or not. We deploy a decision tree to classify adoption based on age, sex, neuter, name, cat/kitten, breed, mix breed dummy, breed certification, hair length, main color, minor color and coat pattern.

```{r, include=FALSE, warning=FALSE, message=FALSE}

#### Prediction Using Decision tree

### tree

colnames(cat1)[colnames(cat1)=="age_at.result"] <- "age"
cat1$breed_1<-as.factor(ifelse(cat1$breed_1=="0", 0, 1))

cat.tree<-cat1 %>% dplyr::select(name,sex,Spay.Neuter,Cat.Kitten,age,cfa_breed,breed_1, breed_mix, 
                   long, coat_pattern,color1, color2, result)


cat.tree$adopt <- as.factor(ifelse(cat.tree$result==1, 1, 0))


tree.cat=tree(adopt~.-result, data=cat.tree)
summary(tree.cat)

```
#### Figure 3   

```{r, echo=FALSE}
plot(tree.cat)
text(tree.cat, pretty=0)


```
&nbsp;
&nbsp;  

The tree we grow is shown in figure 3: top decision node represents if the cat has been neutered. If no, the tree predicts that it will not be adopted though the probability of adoption will differ conditional on if it has a name and if it is older than 52 days. If neuter status is yes, tree predicts no adoption for cats less than 43 days of age. Neutered cats of age 44 days to 182 days are likely to be adopted, and older cats are predicted to be adopted if they have a name.   
&nbsp;
&nbsp; 

We split the original data set into training data and testing data. Building tree from the training data and testing on the test set gives a confusion matrix as below. Misclassification rate is around 15%.   
&nbsp;
&nbsp; 
            adopt
tree.pred    0       1
      0    2442    238
      1    620     2613  
    

We also used cross validation to see if the tree can be pruned as in figure 4. Since the current size is at the bottom of misclass curve, we think that the current tree does not overfit the data thus does not need to be pruned.   

&nbsp;
&nbsp; 

#### Figure 4   

```{r, include=FALSE, warning=FALSE, message=FALSE}


set.seed(1)
train=sample(1:nrow(cat.tree),23500)
tree.cat=tree(adopt~.-result, cat.tree, subset=train)
plot(tree.cat)
text(tree.cat,pretty=0)

tree.pred=predict(tree.cat,cat.tree[-train,],type="class")
with(cat.tree[-train,],table(tree.pred,adopt))
(2442+2613)/(620+238+2442+2613)


### cv and pruning
cv.cat=cv.tree(tree.cat,FUN=prune.misclass)
cv.cat

```

```{r, echo=FALSE}
plot(cv.cat) 
```

Next, we used bagging and random forest to improve the decision tree. Since there are 12 explanatory variables, we loop from 1 to 12 as the number of variables randomly selected at each decision nodes. In each iteration, we bootstrap from original dataset and build many slightly different trees. We take the average of those trees as if we are using majority voting to predict the likelihood that the animal will be adopted. We plotted the mean squared error curve for out-of-bag (all bootstrap samples except itself, similar to leave-one-out) error and test set error.   


Bagging is random forest where we select all 12 variables in each decision node. Bagging mean squared error is shown on the rightmost points in figure 5, which is around 0.11 for OOB and 0.113 for test set. We see that MSE bottom out at 0.105 if we select 3 or 4 variables to split the data in each decision node.    



```{r, include=FALSE, warning=FALSE, message=FALSE}
### bagging and random forest
cat.tree$adopt <- as.numeric(ifelse(cat.tree$result==1, 1, 0))
adopt <- cat.tree$adopt 

oob.err=double(12)
test.err=double(12)


for(mtry in 1:12){
     fit=randomForest(adopt~.-result, data=cat.tree, subset=train, mtry=mtry, ntree=5)
     oob.err[mtry]=fit$mse[5]
     pred=predict(fit,cat.tree[-train,])
     test.err[mtry]=with(cat.tree[-train,],mean((adopt-pred)^2))
     cat(mtry," ")
   }




```



#### Figure 5   

```{r, echo=FALSE}


matplot(1:mtry,cbind(test.err,oob.err),pch=19,col=c("red","blue"),type="b",ylab="Mean Squared Error",
        xlab="number of variables", main="MSE curve for OOB and test set")
legend("topright",legend=c("out-of-bag","test"),pch=19,col=c("red","blue"))


```

We also tried boosting to build 10000 shallow trees and stack them together to reduce the variance. Figure 6 is the variance importance plot by boosting. We see that age, neuter, name, main color and coat pattern are the most important factors. The partial dependence plots are in the appendix.   

```{r, include=FALSE, warning=FALSE, message=FALSE}

### boosting

boost.cat=gbm(adopt~.-result,data=cat.tree[train,], distribution="gaussian",
              n.trees=1000,shrinkage=0.1,interaction.depth=5)
summary<-summary(boost.cat)
summary$var <- factor(summary$var, levels = summary$var[order(summary$rel.inf)])


```


#### Figure 6   

```{r, echo=FALSE}

ggplot(summary, aes(x=var, y=rel.inf)) +
  geom_bar(stat = "identity")+
  coord_flip()+
  labs(title="Variable Importance Plot for Boosting",y="relative influence", x = "variables")



```
Boosting yields an error curve as in figure 7. By creating a sufficiently large number of shallow trees, we could achieve an MSE of 0.103, which is lower than the random forest error marked as the horizontal line.   

```{r, include=FALSE, warning=FALSE, message=FALSE}

n.trees=seq(from=100,to=5000,by=20)
predmat=predict(boost.cat,newdata=cat.tree[-train,],n.trees=n.trees)
dim(predmat)
berr=with(cat.tree[-train,],apply( (predmat-adopt)^2,2,mean))


```

#### Figure 7   

```{r, echo=FALSE}
plot(n.trees,berr,pch=19,ylab="Mean Squared Error", xlab="# Trees",
     main="Boosting Test Error and Bagging Test Error", ylim=c(0.10,0.135))
abline(h=min(test.err),col="red")
```
&nbsp;
&nbsp; 

### Conclusion    
&nbsp;
&nbsp; 


Decision tree gives a misclassification rate of 15% whereas bagging and boosting will improve the model. Bagging yielding an MSE of 0.11 and random forests of 4 variables yields an MSE of 0.105, whereas boosting can yield an MSE of 0.103. Or all the 12 variables, age, name and neuter are the most important factors that affect people’s adoption decision. People favor neutered kitten of age 43 days to 182 days, or older neutered cats with a name. Color and pattern might also have some predictive power and people slightly prefer blueish, white and black cats with smoke, tabby and torbie patterns. The reason why breeds information do not enter the tree might be that there are not enough observation with a certified breed (breeds other than domestic short/long hair).    

Overall, the tree with boosting and random forest model can be used to predict cat adoption result. It also suggests that neuter and age are the two most important factors, and shelter center should neuter the cats at the right age so that it is easier to get them adopted.    



&nbsp;
&nbsp; 

&NewLine;

</br>

&NewLine;

</br>
&NewLine;

</br>

&NewLine;

</br>


### Appendix   

Boosting - partial dependent plot of the important variables.



```{r, echo=FALSE}

plot(boost.cat,i="age", main="partial dependence plot: age", ylab="adoption")
plot(boost.cat,i="color1", main="partial dependence plot: main color", ylab="adoption")
plot(boost.cat,i="coat_pattern", main="partial dependence plot: coat pattern", ylab="adoption")

```

The disease and death rates are high (reflected in higher frequency in euthanasia and die) for newborn kittens. People tend to adopt slightly older kittens and the partial dependence is stable over age. Partial dependence drops once the age become larger than 10 years.   

Main color and coat pattern also affect people’s decision to adopt a cat. Roughly speaking, blueish, white and black are preferred, and smoke, tabby and tortie-tabby are preferred.    

